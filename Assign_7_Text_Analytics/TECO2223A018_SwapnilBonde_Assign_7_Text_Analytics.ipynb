{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c2b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Text Analytics\n",
    "1. Extract Sample document and apply following document preprocessing methods:\n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of document by calculating Term Frequency and Inverse Document\n",
    "Frequency.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd47bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1) Text Preprocessing:\n",
    "    1. Sentence/word tokenization\n",
    "    2. Removing Stopwords\n",
    "    3. Stemming\n",
    "    4. Lematization\n",
    "    \n",
    "2) TF_IDF\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d54fa48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "695b59c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/ubuntu/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/.local/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42219378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2fe8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71d50e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3ec672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this df is not considered in this notebook, but can be used for preprocessing purposes\n",
    "\n",
    "# df = pd.read_csv('https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/amazon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31641ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7307d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample text\n",
    "\n",
    "text=\"\"\"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\n",
    "The sky is pinkish-blue. You shouldn't eat cardboard\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9bbea89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and city is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard\"]\n"
     ]
    }
   ],
   "source": [
    "tokenized = sent_tokenize(text)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15b3db44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_words = word_tokenize(text)\n",
    "print(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1828b9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2affc29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d483ed7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'under', 'am', 'have', 'did', 'out', 'so', 'aren', 'by', 'she', 'some', 'do', 'should', 'there', 'on', 'when', 'during', 'down', 'from', 'you', 'at', 'then', 'are', 'up', 't', 'they', 'doesn', 're', \"doesn't\", 'ours', \"shan't\", 'himself', 'has', \"weren't\", 'does', 'to', 'weren', 'mightn', \"you'd\", 'shouldn', 'not', 'those', 'below', 'after', 'whom', 'before', 'him', 'shan', 'don', 'further', 'a', 'that', 'more', 'these', 'into', 'as', 'was', 'the', 'while', 'just', 'ourselves', \"needn't\", 'until', 'such', 'but', 'isn', 'for', 'here', 'it', 'myself', \"mustn't\", \"you're\", \"shouldn't\", 'didn', 'doing', 'with', 'o', 'yours', 'y', \"hasn't\", \"she's\", 'which', \"hadn't\", 'because', 'he', 'm', 'its', 'themselves', 'very', \"mightn't\", 'above', 'i', 'once', 'my', 'wasn', 'most', 'all', 'an', 'wouldn', 'll', \"wasn't\", 'your', \"aren't\", 'their', \"you've\", \"you'll\", 'nor', 'her', 'hers', 'hasn', 'each', 'needn', 'we', 'same', 'can', 've', \"wouldn't\", 'his', 'and', 'both', 'any', 'in', \"should've\", 'd', \"don't\", 'own', 'only', \"couldn't\", \"isn't\", \"won't\", 'mustn', 'of', \"it's\", 'won', 'against', \"didn't\", 'or', 'about', 'theirs', 'our', 'me', 'itself', 'through', 'herself', 'who', 'had', 'how', \"haven't\", 'over', 'be', 'few', 'being', 'why', 'again', 'too', 'than', 'this', 'other', 'were', 'if', 'where', 'is', 'yourselves', 'now', 'no', 's', 'having', 'off', 'couldn', 'haven', 'yourself', \"that'll\", 'ain', 'hadn', 'what', 'them', 'been', 'between', 'ma', 'will'}\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57b524f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered_Sentence: \n",
      "['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "filtered_sent = []\n",
    "\n",
    "for w in tokenized_words:\n",
    "    if w not in stopwords:\n",
    "        filtered_sent.append(w)\n",
    "print(\"Filtered_Sentence: \")        \n",
    "print(filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2325e2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Sentence: ['hello', 'mr.', 'smith', ',', 'today', '?', 'the', 'weather', 'great', ',', 'citi', 'awesom', '.', 'the', 'sky', 'pinkish-blu', '.', 'you', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_words=[]\n",
    "for w in filtered_sent:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "print(\"Stemmed Sentence:\",stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6fc299f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "925964da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized Sentence:  ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_words = []\n",
    "\n",
    "for w in filtered_sent:\n",
    "    lemmatized_words.append(lem.lemmatize(w))\n",
    "    \n",
    "print(\"lemmatized Sentence: \", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7c825b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_tag(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd9d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4b2700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b907450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a4bb63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello | INTJ | interjection | UH interjection\n",
      "\n",
      "Mr. | PROPN | proper noun | NNP noun, proper singular\n",
      "\n",
      "Smith | PROPN | proper noun | NNP noun, proper singular\n",
      "\n",
      ", | PUNCT | punctuation | , punctuation mark, comma\n",
      "\n",
      "how | SCONJ | subordinating conjunction | WRB wh-adverb\n",
      "\n",
      "are | AUX | auxiliary | VBP verb, non-3rd person singular present\n",
      "\n",
      "you | PRON | pronoun | PRP pronoun, personal\n",
      "\n",
      "doing | VERB | verb | VBG verb, gerund or present participle\n",
      "\n",
      "today | NOUN | noun | NN noun, singular or mass\n",
      "\n",
      "? | PUNCT | punctuation | . punctuation mark, sentence closer\n",
      "\n",
      "The | DET | determiner | DT determiner\n",
      "\n",
      "weather | NOUN | noun | NN noun, singular or mass\n",
      "\n",
      "is | AUX | auxiliary | VBZ verb, 3rd person singular present\n",
      "\n",
      "great | ADJ | adjective | JJ adjective (English), other noun-modifier (Chinese)\n",
      "\n",
      ", | PUNCT | punctuation | , punctuation mark, comma\n",
      "\n",
      "and | CCONJ | coordinating conjunction | CC conjunction, coordinating\n",
      "\n",
      "city | NOUN | noun | NN noun, singular or mass\n",
      "\n",
      "is | AUX | auxiliary | VBZ verb, 3rd person singular present\n",
      "\n",
      "awesome | ADJ | adjective | JJ adjective (English), other noun-modifier (Chinese)\n",
      "\n",
      ". | PUNCT | punctuation | . punctuation mark, sentence closer\n",
      "\n",
      "\n",
      " | SPACE | space | _SP whitespace\n",
      "\n",
      "The | DET | determiner | DT determiner\n",
      "\n",
      "sky | NOUN | noun | NN noun, singular or mass\n",
      "\n",
      "is | AUX | auxiliary | VBZ verb, 3rd person singular present\n",
      "\n",
      "pinkish | NOUN | noun | NN noun, singular or mass\n",
      "\n",
      "- | PUNCT | punctuation | HYPH punctuation mark, hyphen\n",
      "\n",
      "blue | ADJ | adjective | JJ adjective (English), other noun-modifier (Chinese)\n",
      "\n",
      ". | PUNCT | punctuation | . punctuation mark, sentence closer\n",
      "\n",
      "You | PRON | pronoun | PRP pronoun, personal\n",
      "\n",
      "should | AUX | auxiliary | MD verb, modal auxiliary\n",
      "\n",
      "n't | PART | particle | RB adverb\n",
      "\n",
      "eat | VERB | verb | VB verb, base form\n",
      "\n",
      "cardboard | NOUN | noun | NN noun, singular or mass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \"|\", token.pos_,\"|\", spacy.explain(token.pos_),\"|\",token.tag_, spacy.explain(token.tag_))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb804f0",
   "metadata": {},
   "source": [
    "## TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88c58fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8af15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e6cec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object\n",
    "tfidf = TfidfVectorizer()\n",
    " \n",
    "# get tf-df values\n",
    "result = tfidf.fit_transform(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "99dd3863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "idf values:\n",
      "awesome : 3.3978952727983707\n",
      "blue : 3.3978952727983707\n",
      "cardboard : 3.3978952727983707\n",
      "city : 3.3978952727983707\n",
      "eat : 3.3978952727983707\n",
      "great : 3.3978952727983707\n",
      "hello : 3.3978952727983707\n",
      "mr : 3.3978952727983707\n",
      "pinkish : 3.3978952727983707\n",
      "sky : 3.3978952727983707\n",
      "smith : 3.3978952727983707\n",
      "the : 2.992430164690206\n",
      "today : 3.3978952727983707\n",
      "weather : 3.3978952727983707\n",
      "you : 3.3978952727983707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# get idf values\n",
    "print('\\nidf values:')\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):\n",
    "\tprint(ele1, ':', ele2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e036ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word indexes:\n",
      "{'hello': 6, 'mr': 7, 'smith': 10, 'today': 12, 'the': 11, 'weather': 13, 'great': 5, 'city': 3, 'awesome': 0, 'sky': 9, 'pinkish': 8, 'blue': 1, 'you': 14, 'eat': 4, 'cardboard': 2}\n",
      "\n",
      "tf-idf value:\n",
      "  (0, 6)\t1.0\n",
      "  (1, 7)\t1.0\n",
      "  (2, 10)\t1.0\n",
      "  (4, 12)\t1.0\n",
      "  (6, 11)\t1.0\n",
      "  (7, 13)\t1.0\n",
      "  (8, 5)\t1.0\n",
      "  (10, 3)\t1.0\n",
      "  (11, 0)\t1.0\n",
      "  (13, 11)\t1.0\n",
      "  (14, 9)\t1.0\n",
      "  (15, 1)\t0.7071067811865475\n",
      "  (15, 8)\t0.7071067811865475\n",
      "  (17, 14)\t1.0\n",
      "  (19, 4)\t1.0\n",
      "  (20, 2)\t1.0\n",
      "\n",
      "tf-idf values in matrix form:\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.70710678 0.         0.         0.         0.\n",
      "  0.         0.         0.70710678 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# get indexing\n",
    "print('\\nWord indexes:')\n",
    "print(tfidf.vocabulary_)\n",
    "\n",
    "# display tf-idf values\n",
    "print('\\ntf-idf value:')\n",
    "print(result)\n",
    "\n",
    "# in matrix form\n",
    "print('\\ntf-idf values in matrix form:')\n",
    "print(result.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7fbe4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
